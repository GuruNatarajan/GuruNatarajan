{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing session and create \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JARVIS:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x197865414c0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark=spark.read.csv(\"test1.csv\")\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|   _c0| _c1| _c2|   _c3|\n",
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "|   sam|  31|  10| 20000|\n",
      "|   ram|  30|   8| 20000|\n",
      "| vikky|  29|   4| 15000|\n",
      "| anand|  18|  17|  null|\n",
      "|mahesh|null|null| 20000|\n",
      "|  null|  20|   3| 20000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|   sam|  31|  10| 20000|\n",
      "|   ram|  30|   8| 20000|\n",
      "| vikky|  29|   4| 15000|\n",
      "| anand|  18|  17|  null|\n",
      "|mahesh|null|null| 20000|\n",
      "|  null|  20|   3| 20000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset\n",
    "spark.read.option(\"header\",\"true\").csv(\"test1.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: string, exp: string, salary: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option(\"header\",\"true\").csv(\"test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- exp: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the data and check the schema\n",
    "df_spark=spark.read.option(\"header\",\"true\").csv(\"test1.csv\",inferSchema=True)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'exp', 'salary']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='sam', age=31, exp=10, salary=20000)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='sam', age=31, exp=10, salary=20000),\n",
       " Row(name='ram', age=30, exp=8, salary=20000),\n",
       " Row(name='vikky', age=29, exp=4, salary=15000),\n",
       " Row(name='anand', age=18, exp=17, salary=None)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|   sam|\n",
      "|   ram|\n",
      "| vikky|\n",
      "| anand|\n",
      "|mahesh|\n",
      "|  null|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.select([\"name\",\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|   sam|\n",
      "|   ram|\n",
      "| vikky|\n",
      "| anand|\n",
      "|mahesh|\n",
      "|  null|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_spark.select(\"name\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  name| age|\n",
      "+------+----+\n",
      "|   sam|  31|\n",
      "|   ram|  30|\n",
      "| vikky|  29|\n",
      "| anand|  18|\n",
      "|mahesh|null|\n",
      "|  null|  20|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select([\"name\",\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+-----------------+----------------+\n",
      "|summary| name|              age|              exp|          salary|\n",
      "+-------+-----+-----------------+-----------------+----------------+\n",
      "|  count|    5|                5|                5|               5|\n",
      "|   mean| null|             25.6|              8.4|         19000.0|\n",
      "| stddev| null|6.107372593840988|5.594640292279746|2236.06797749979|\n",
      "|    min|anand|               18|                3|           15000|\n",
      "|    max|vikky|               31|               17|           20000|\n",
      "+-------+-----+-----------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.describe().show() # the null is because the pyspark will take the string columns also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding column in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spark.withColumn(\"Exp after 2 yrs\",df_spark[\"exp\"]+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+---------------+\n",
      "|  name| age| exp|salary|Exp after 2 yrs|\n",
      "+------+----+----+------+---------------+\n",
      "|   sam|  31|  10| 20000|             12|\n",
      "|   ram|  30|   8| 20000|             10|\n",
      "| vikky|  29|   4| 15000|              6|\n",
      "| anand|  18|  17|  null|             19|\n",
      "|mahesh|null|null| 20000|           null|\n",
      "|  null|  20|   3| 20000|              5|\n",
      "+------+----+----+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff_pyspark=df_spark.withColumn(\"Exp after 2 yrs\",df_spark[\"exp\"]+2)\n",
    "dff_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spark.drop(\"Exp after 2 yrs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|   sam|  31|  10| 20000|\n",
      "|   ram|  30|   8| 20000|\n",
      "| vikky|  29|   4| 15000|\n",
      "| anand|  18|  17|  null|\n",
      "|mahesh|null|null| 20000|\n",
      "|  null|  20|   3| 20000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff_pyspark=df_spark.drop(\"exp after 2 yrs\")# note: the (e) is small \n",
    "dff_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|new name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|     sam|  31|  10| 20000|\n",
      "|     ram|  30|   8| 20000|\n",
      "|   vikky|  29|   4| 15000|\n",
      "|   anand|  18|  17|  null|\n",
      "|  mahesh|null|null| 20000|\n",
      "|    null|  20|   3| 20000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff_pyspark.withColumnRenamed(\"name\",\"new name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping the nan or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| name|age|exp|salary|\n",
      "+-----+---+---+------+\n",
      "|  sam| 31| 10| 20000|\n",
      "|  ram| 30|  8| 20000|\n",
      "|vikky| 29|  4| 15000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|   sam|  31|  10| 20000|\n",
      "|   ram|  30|   8| 20000|\n",
      "| vikky|  29|   4| 15000|\n",
      "| anand|  18|  17|  null|\n",
      "|mahesh|null|null| 20000|\n",
      "|  null|  20|   3| 20000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how== \"any\" if the row have even 1 nan or null values it'll remove the row\n",
    "# how== \"all\" a row must contain entirely nan or null values\n",
    "dff_pyspark.na.drop(how=\"any\",thresh=2).show() # thresh value says ,atleast (2) non nan values should present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|   sam|  31|  10| 20000|\n",
      "|   ram|  30|   8| 20000|\n",
      "| vikky|  29|   4| 15000|\n",
      "| anand|  18|  17|  null|\n",
      "|mahesh|null|null| 20000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset is to specify the column \n",
    "dff_pyspark.na.drop(how=\"any\",subset=[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling the nan or null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'exp', 'salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'exp', 'salary']]\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|  name| age| exp|salary|age_imputed|exp_imputed|salary_imputed|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|   sam|  31|  10| 20000|         31|         10|         20000|\n",
      "|   ram|  30|   8| 20000|         30|          8|         20000|\n",
      "| vikky|  29|   4| 15000|         29|          4|         15000|\n",
      "| anand|  18|  17|  null|         18|         17|         20000|\n",
      "|mahesh|null|null| 20000|         29|          8|         20000|\n",
      "|  null|  20|   3| 20000|         20|          3|         20000|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(dff_pyspark).transform(dff_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| name|age|exp|salary|\n",
      "+-----+---+---+------+\n",
      "|  sam| 31| 10| 20000|\n",
      "|  ram| 30|  8| 20000|\n",
      "|vikky| 29|  4| 15000|\n",
      "|anand| 18| 17| 18000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_spark=spark.read.csv(\"test2.csv\",header=True,inferSchema=True)\n",
    "df2_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+\n",
      "|name|age|exp|salary|\n",
      "+----+---+---+------+\n",
      "| sam| 31| 10| 20000|\n",
      "| ram| 30|  8| 20000|\n",
      "+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_spark.filter(\"salary>=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| sam| 31|\n",
      "| ram| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_spark.filter(\"salary>=20000\").select([\"name\",\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+\n",
      "|name|age|exp|salary|\n",
      "+----+---+---+------+\n",
      "| sam| 31| 10| 20000|\n",
      "| ram| 30|  8| 20000|\n",
      "+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_spark.filter((df2_spark[\"salary\"]>=20000) |             # here we can use and(&) ,or(|) and other operators\n",
    "                 (df2_spark[\"salary\"]<=10000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| name|age|exp|salary|\n",
      "+-----+---+---+------+\n",
      "|vikky| 29|  4| 15000|\n",
      "|anand| 18| 17| 18000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_spark.filter(~(df2_spark[\"salary\"]>=20000)).show()  # (~) this not operator or here we can use it as inverse operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and Aaggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_spark=spark.read.csv(\"test3.csv\",header=True,inferSchema=True)\n",
    "df3_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## groupby\n",
    "# to finnd the maximum salary\n",
    "df3_spark.groupBy(\"name\").sum().show() #sum()=sum(salary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_spark.groupBy(\"departments\").sum().show() # instead sum we can use max,min.mean and etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| departments|max(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      10000|\n",
      "|    Big Data|       5000|\n",
      "|Data Science|      20000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_spark.groupBy(\"departments\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     name|max(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      20000|\n",
      "|    Sunny|      10000|\n",
      "|    Krish|      10000|\n",
      "|   Mahesh|       4000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_spark.groupBy(\"name\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2efd24cad365aec74b3bceb0c870f58e0a056936251985cf68090e984f5841d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
